# -*- coding: utf-8 -*-
"""Llava_Whisper_Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IA7DGCLAFFclCVay9XlW3YJtJzUab9xx

# Important Libraries

1. transformers: Load models and handle NLP tasks.  
2. bitsandbytes: Efficient memory usage during training.
3. accelerate: Speed up training and inference.
4. gradio: Create interactive UIs.
5. gTTS: Convert text to speech (Google Text-to-Speech).
6. whisper: Recognize speech (OpenAI's).
"""

#@title Import Required Libraries
import os
import re
import nltk
import torch
import base64
import whisper
import datetime
import requests
import numpy as np
import gradio as gr
from gtts import gTTS
from PIL import Image
from nltk.tokenize import sent_tokenize
from transformers import BitsAndBytesConfig, pipeline
from IPython.display import Markdown, display

nltk.download('punkt')
import warnings
warnings.filterwarnings('ignore')

"""# Model Pipeline

LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture.
"""

#@title Quantize the Config for Model pipeline

quantization_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_compute_dtype = torch.float16)

MODEL_ID = "llava-hf/llava-1.5-7b-hf"

pipe = pipeline("image-to-text",
                model = MODEL_ID,
                model_kwargs = {"quantization_config" : quantization_config})

#@title Check for GPU Enabled
torch.cuda.is_available()

Device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"torch using {torch.__version__}, ({Device})")

"""# Test the Model

"""

#@title Load the Image
image_path = '/content/Evidence-Based_2801e9e5-edd5-4d76-b66f-57a55bb780c0_1024x1024.png'
image = Image.open(image_path)
image

#@title Create the Prompt Template

max_tokens = 250
prompt_template = """
Describe the Image using as much as detailed as possible.
Generate a clear and helpful summary or answer related to the image based on your description.
As You are a Helpful Assistant who is able to answer questions about the image.
What is the Image all about ?
Now generate a helpful answer about the image
"""
prompt = "User: <image>\n" + prompt_template + "\nAssistant: \n\n"

#@title Generate the Response from the Model
response = pipe(image,
                prompt = prompt,
                generate_kwargs = {"max_new_tokens" : max_tokens})

for sent in sent_tokenize(response[0]["generated_text"]):
  print(sent)

"""# Model Deploy"""

#@title Loading the Model from Whisper
model = whisper.load_model("medium", device = Device)

print(
    f"Model is {'multilingual' if model.is_multilingual else 'English only'} "
    f"and has {sum(n.numel() for n in model.parameters()):,} parameters."
     )

#@title Logger File
tstamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tstamp = str(tstamp).replace(" ","_")
logfile = f"log_{tstamp}.txt"

# define the fuction to write
def writehistory(text):
  with open(logfile, "a", encoding = "utf-8") as f:
    f.write(text +"\fake")
    f.write("\n")
  f.close()

def image_2_text(input_image, input_text):
    # Load the image
    image = Image.open(input_image)

    writehistory(f"Input Image: {input_image} - Type: {type(input_image)} - Dir: {os.path.dirname(input_image)}")

    # Determine the prompt Instructions based on the type of input_text
    if type(input_text) == tuple:
        prompt_template = """
        Describe the Image using as much detail as possible.
        Generate a clear and helpful summary or answer related to the image based on your description.
        As you are a Helpful Assistant who is able to answer questions about the image.
        What is the Image all about ?
        Now generate a helpful answer about the image.
        """ + ' '.join(input_text)
    else:
        prompt_template = """
        Act as an expert in imagery descriptive analysis. Using as much detail as possible from the image, respond to the following prompt:
        """ + input_text

    writehistory(f"Prompt: {prompt_template}")

    # Format the final prompt
    prompt = "USER: <image>\n" + prompt_template + "\nASSISTANT:\n"

    # Generate the Response from the model
    response = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 250})

    # Extract the Response text Properly
    if response is not None and len(response[0]["generated_text"]) > 0:
        match = re.search(r"ASSISTANT:\s*(.*)", response[0]["generated_text"])
        if match:
            # Extract the text after "ASSISTANT:"
            reply = match.group(1).strip()
        else:
            reply = "No Response Found"
    else:
        reply = "No Response Generated"

    return reply

#@title Trancribe the Audio

def transcribe(audio):
    # Check if the audio input is None or empty
    if audio is None or audio == "":
        return '', '', None  # Return empty strings and None for Audio File

    # Load and Process the Audio
    audio_data = whisper.load_audio(audio)
    audio_data = whisper.pad_or_trim(audio_data)

    # Generate the Mel Spectrogram
    mel = whisper.log_mel_spectrogram(audio_data).to(model.device)

    # Detect Language
    _, probs = model.detect_language(mel)
    detected_language = max(probs, key=probs.get)
    writehistory(f"Detected Language: {detected_language}")

    # Decode the Audio
    options = whisper.DecodingOptions()
    results = whisper.decode(model, mel, options)
    results_text = results.text

    return results_text, detected_language  # Return text and detected language if needed

#@title Text-to-Speech Generation
def text_2_speech(text, file_path):
    language = 'en'

    audio_obj = gTTS(text = text,
                     lang = language,
                     slow = False)
    audio_obj.save(file_path)

    return file_path

#@title Before Running the Pipeline
import locale
print(locale.getlocale())

locale.getpreferredencoding = lambda: "UTF-8"

"""#FFmpeg Command: Create a 10-second silent MP3 file.

* f lavfi: Use a filter as an input.




* i anullsrc=r=44100:cl=mono: Generate silent audio with a sample rate of 44100 Hz in mono.
* t 10: Set the duration of the audio to 10 seconds.
* q:a 9: Set the audio qu
ality level to 9 (lower quality, smaller file size).
* acodec libmp3lame: Encode the audio using the MP3 format.
* Temp.mp3: Output file name.
"""

# Run the FFmpeg Command
!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3

#@title Process The Inputs

def process_inputs(audio_path, image_path, confidence_threshold, include_image_analysis, selected_option):
    # Process the audio file
    speech_to_text_output = transcribe(audio_path)  # Assuming transcribe() is defined

    # Prepare the AI response based on the image
    if include_image_analysis and image_path:
        chatgpt_output = image_2_text(image_path, speech_to_text_output)  # Assuming image_2_text() is defined
    else:
        chatgpt_output = "Image analysis not included."

    # Convert the AI output to Speech and save it as an audio file
    processed_audio_path = text_2_speech(chatgpt_output, "Temp.mp3")  # Assuming text_2_speech() is defined

    # Debug info or additional processing can be handled here
    debug_info = {
        "confidence_threshold": confidence_threshold,
        "selected_option": selected_option,
        "include_image_analysis": include_image_analysis
    }

    return speech_to_text_output, chatgpt_output, processed_audio_path, None, debug_info

import gradio as gr

iface = gr.Interface(
    fn=process_inputs,
    inputs=[
        gr.Audio(sources="microphone", type="filepath"),
        gr.Image(type="filepath"),
        gr.Slider(minimum=0, maximum=10, value=5, label="Confidence Threshold"),
        gr.Checkbox(label="Include Image Analysis"),
        gr.Dropdown(choices=["Option 1", "Option 2", "Option 3"], label="Select Option")
    ],
    outputs=[
        gr.Textbox(label="Speech to Text"),
        gr.Textbox(label="AI Response"),
        gr.Audio(label="Generated Audio"),
        gr.Image(label="Image Analysis Result"),
        gr.JSON(label="Debug Info")
    ],
    title="LLM Powered Voice Assistance for Multimodal Data",
    description="Upload your image and interact via voice input and audio responses. Adjust settings and view additional outputs.",
    theme="default",  # Change to a valid theme if needed
    live=False  # Set to False to include the submit button
)

# Launch the interface
iface.launch(debug = True)